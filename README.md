# MIPT Hackathon VK -- Промежуточный результат

1. Data_Processing:
   * Получение классических features и их визуализация
   * Подготовка embeddings на основе различных transformer-encoder, в том числе и BERT'а
2. DNN_LTR:
   * Построение torch модели из Dense слоев
   * Проверка качества
3. Catboost_LTR:
   * Прогон Catboost на основе текстов и классических features
   * Прогон Catboost на основе текстов и классических features + embeddings
   * Результаты оказались одинаковыми

### Предварительные выводы:

  a) Catboost и нейронка показывают одинаковый результат. Разные значения NDCG в файлах обусловлены использованием разных функций для обсчета NDCG. Но при применении одной - результаты оказались одинаковыми.
  -> Планируется сделать stacking нескольких моделей, дающих близкие результаты (+ XGBoost, RandomForest, BertForSentenceClassification модели с huggingface)
  
  б) После ранжирования Catboost'ом построили гистограммы на основе NDCG для валидационной выборки. Оказалось, что больша'я часть данных имеют высокий NDCG, а часть распределена около точки максимальной энтропии (посередине между minNDCG и maxNDCG=1).
  -> Планируется изучить в чем различие между 'хорошими' данными и 'случайными'
